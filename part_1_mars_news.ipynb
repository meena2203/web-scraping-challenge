{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12 Challenge\n",
    "## Deliverable 1: Scrape Titles and Preview Text from Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "!pip install bs4\n",
    "! pip install splinter\n",
    "!pip install webdriver_manager\n",
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import jason\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup splinter (opens automated chrome browser)\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Visit the Website\n",
    "\n",
    "1. Use automated browsing to visit the [Mars NASA news site](https://redplanetscience.com). Inspect the page to identify which elements to scrape.\n",
    "\n",
    "      > **Hint** To identify which elements to scrape, you might want to inspect the page by using Chrome DevTools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Mars NASA news site: 'https://redplanetscience.com' to scrape\n",
    "url = 'https://redplanetscience.com'\n",
    "    \n",
    "# In automated browser, go to the above url\n",
    "browser.visit(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the Website\n",
    "\n",
    "Create a Beautiful Soup object and use it to extract text elements from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Beautiful Soup object\n",
    "html = browser.html\n",
    "soup = soup(html, 'html.parser')\n",
    "\n",
    "# Check the type\n",
    "#type(soup)\n",
    "\n",
    "# Print formatted version of soup\n",
    "#print(soup.prettify())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the title and preview of the news articles\n",
    "mars_articles = soup.find_all('div', class_= ['content_title', 'article_teaser_body'])\n",
    "print(mars_articles)\n",
    "\n",
    "# for each in mars_articles:\n",
    "#     print(each.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Store the Results\n",
    "\n",
    "Extract the titles and preview text of the news articles that you scraped. Store the scraping results in Python data structures as follows:\n",
    "\n",
    "* Store each title-and-preview pair in a Python dictionary. And, give each dictionary two keys: `title` and `preview`. An example is the following:\n",
    "\n",
    "  ```python\n",
    "  {'title': \"Mars Rover Begins Mission!\", \n",
    "        'preview': \"NASA's Mars Rover begins a multiyear mission to collect data about the little-explored planet.\"}\n",
    "  ```\n",
    "\n",
    "* Store all the dictionaries in a Python list.\n",
    "\n",
    "* Print the list in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the dictionaries\n",
    "articles = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the text elements\n",
    "# Extract the title and preview text from the elements\n",
    "# Store each title and preview pair in a dictionary\n",
    "# Add the dictionary to the list\n",
    "\n",
    "for each in mars_articles:\n",
    "    title = each.find('content_title')\n",
    "    preview = each.find('article_teaser_body')\n",
    "    articles.append({'title': title, 'preview': preview})\n",
    "\n",
    "articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list to confirm success\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Step 4: Export the Data\n",
    "\n",
    "Optionally, store the scraped data in a file or database (to ease sharing the data with others). To do so, export the scraped data to either a JSON file or a MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to JSON\n",
    "\n",
    "json_data = json.dumps(articles)\n",
    "print(json_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to MongoDB\n",
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Define database and collection\n",
    "mars_news_db = client.news_db\n",
    "collection = mars_news_db.articles\n",
    "\n",
    "# Insert dictionary into MongoDB as a document\n",
    "collection.insert_many(articles)\n",
    "\n",
    "# Display the MongoDB records created above\n",
    "records = mars_news_db.articles.find()\n",
    "for record in records:\n",
    "    print(record)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data-class-mars-challenge.s3.amazonaws.com/Mars/index.html'\n",
    "\n",
    "# In automated browser, go to the above url\n",
    "browser.visit(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ResultSet' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract all rows of the data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Create a Beautiful Soup object\u001b[39;00m\n\u001b[0;32m      3\u001b[0m html \u001b[38;5;241m=\u001b[39m browser\u001b[38;5;241m.\u001b[39mhtml\n\u001b[1;32m----> 4\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# # Check the type\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# type(soup)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Extract the table (only one table)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'ResultSet' object is not callable"
     ]
    }
   ],
   "source": [
    "# Extract all rows of the data\n",
    "# Create a Beautiful Soup object\n",
    "html = browser.html\n",
    "soup = soup(html, 'html.parser')\n",
    "\n",
    "# # Check the type\n",
    "# type(soup)\n",
    "\n",
    "# # Print formatted version of soup\n",
    "# print(soup.prettify())\n",
    "\n",
    "# Extract the table (only one table)\n",
    "table = soup.find('table')\n",
    "\n",
    "    \n",
    "# Extract all rows of the table\n",
    "table_rows = table.find_all('tr')\n",
    "\n",
    "# Extract the headr data 'th' from the first row of the table\n",
    "column_name = table_rows.find_all('th')\n",
    "print(header_data)\n",
    "\n",
    "\n",
    "# Create an empty list\n",
    "row_list = []\n",
    "\n",
    "# Iterate through each row of the table to grab the row data 'td'\n",
    "# first list will be empty\n",
    "\n",
    "for tr in table_rows:\n",
    "    row_data = tr.find_all('td')\n",
    "    row_list.append(row_data)    \n",
    "#     row_list = [i.text for i in td]\n",
    "    print(row_list)\n",
    "\n",
    "\n",
    "# Create a Pandas DataFrame by using the list of rows and a list of the column names\n",
    "#mars_weather_df = pd.DataFrame([row_list, columns=column_name])   \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
